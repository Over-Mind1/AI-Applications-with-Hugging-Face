{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b45e45c8",
   "metadata": {
    "id": "b45e45c8"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2159974",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 406,
     "referenced_widgets": [
      "259196b6c37a49cb8aba8b78fe1ad0f2",
      "34fab507f0384d56b204be2e86cb1e63",
      "5b90efb7f273442e8abe4b234ff19d98",
      "088f8bd7b783449ba663ca22a28aa00e",
      "75ae775d69c34247881933abcf72a153",
      "3f568c68818540b59e96845c8a744cd1",
      "c9eb487fdfc24a4c9bd4d6ba0ee189cb",
      "31754bfdca4c4ac3a1582b731858f308",
      "3b57ea9593ac4449b6449b4d825c1ff8",
      "53e1292886c74ecda0d37f5a08a9d063",
      "768aba3403eb43d6839239ef693aa8ac",
      "c42d5e730e594bff8619cb1f5760ab44",
      "1ef6ac16010449519776ac3f4109aabd",
      "8640d5b06ddd464eb7cffccdb94e01ab",
      "2c1ddd324b934862aa0811b4e6ec0f78",
      "11f65927aca84b2697fb40c1e8407c67",
      "d7a39c21a97643b2afc3ae3adb2d6c0e",
      "9d39994fb6e548048d9cb4fe7e8f36e4",
      "cc3f7717dcd2444285597caae71e7e36",
      "068989191c954ac8ae93535fc563341c",
      "2584ec1961ec4741a921d47569db082a",
      "181c6dc5955d47e8b54bbee74d35e490",
      "4d2dd2de03b64493be906e4e1fefa232",
      "6cbff223077f4f6498ce4734460bb810",
      "dae2133bb424468d9c0be130df162815",
      "f814bb75c450418ab37c4f2c4b68d884",
      "0ac6662877ed4fb78b093cecbf86a183",
      "883a584dc92d440ab4f53005485dcdca",
      "e75406f909bd4601aaa7b53a379953e8",
      "426bf055dab948f284f7c1878bfd3957",
      "60fe443991e441c4b86b073f7b5b5ef6",
      "89fafdf482154e2b888adc1fa96828ab",
      "852076dc26b340cf90de5fbae6b96a5a",
      "ecdb636b7395405f99cbe3af37bd78c6",
      "edbc513dbdcb458dad8feba416708a81",
      "e6603419fd9248f4b29aa82a5fff178c",
      "726da3204e19416d8d00492dbe2e9f64",
      "475b258197fe470fb0398ad12f322668",
      "0beb7b5532a745538ee037047a28a180",
      "c5d0cf5f0b044529b0b586865fc38214",
      "352ec9f0c73949dd805dbf6bca3677b9",
      "4290108694ae491996c7dc4a98b25170",
      "d00aa3d30512435dbf941345ae3f3969",
      "42a27bbd47d44090bf89041d579abcfc",
      "89b93aa1f0344071b14b7854079fed0a",
      "0272ebe864c44d5ca628d751e0b4efd9",
      "7ea2dd8c191e4e04a59a73b1c35ba406",
      "f76d13f64ad94afebfba816465ebfd61",
      "f5810875371243e98e367d3808bdeba0",
      "10c0e62870d04d469c16e91b8f23cbdc",
      "1f21bbd721294495a4c74f69aa1497ea",
      "d161688b43c644a98d5e55d075f1c930",
      "5240906c1fb54dbe99fd158a9b8f6fa2",
      "95b86eee38de4c1bbc04c55923dff784",
      "885a34fc59ca452ba9561e3c3c35b196",
      "c79d01d94e304e2182237971f811589b",
      "e50141ea50d242f1b4ddb4a37522c4cd",
      "9396ce20d1ac491199f90df2351be491",
      "f3ca2a0fa48a4593830d81d5904b8ae3",
      "fe110365b9704e6c972672e43045f8f0",
      "338264107df7427990013da109f339bb",
      "92d9302f3dd341848ce8657138cddc8e",
      "5301888fac4f4249bb53bfe17a10fa5c",
      "3c0d4bfe4e6b48ae9896e2154d50a9ca",
      "cd61449aecee4cab8f1d92753c092711",
      "c7af07d1ad1d4a10bd5ca8b1faf12868",
      "7bb5a50c1d894c76813ddbcf27247dce",
      "3acc8f549d9d40ce9e7b6a95dcc026b1",
      "379f417b79394b6c9cc22dcbae0a687e",
      "7c744b81fe7a4748a7699c6aa456d71e",
      "fb60533a17e54ed0a7e4686906358a81",
      "7bc9494d3b77426bb4eb93b062e98428",
      "e978805e309e4eabbb14661b53d8b6f0",
      "f522d88ff8c74ab8ac47057b33c8f1d6",
      "f8bc2a3c43dc48619d71e7e93e9f5ac1",
      "0c79817ced2c4893a8e367a0e5b5a7e2",
      "0f6af71990ce4334af3311d6268033f1"
     ]
    },
    "id": "f2159974",
    "outputId": "0ef3be57-f542-4bc1-e661-0576c8ab9b65"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "259196b6c37a49cb8aba8b78fe1ad0f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/726 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c42d5e730e594bff8619cb1f5760ab44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d2dd2de03b64493be906e4e1fefa232",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecdb636b7395405f99cbe3af37bd78c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89b93aa1f0344071b14b7854079fed0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c79d01d94e304e2182237971f811589b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bb5a50c1d894c76813ddbcf27247dce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "The model 'Qwen3ForCausalLM' is not supported for text2text-generation. Supported models are ['PeftModelForSeq2SeqLM', 'BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'GraniteSpeechForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'Qwen2AudioForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'T5GemmaForConditionalGeneration', 'UMT5ForConditionalGeneration', 'VoxtralForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n"
     ]
    }
   ],
   "source": [
    "Qwen3_bot= pipeline(task=\"text2text-generation\",\n",
    "                       model=\"Qwen/Qwen3-0.6B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4727ee52",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4727ee52",
    "outputId": "c2eb71c9-f959-44dc-f928-daaa13c76e7a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [12555, 374, 16391], 'attention_mask': [1, 1, 1]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_message=\"what is transformer\"\n",
    "Qwen3_bot.tokenizer(user_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "da71aded",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "da71aded",
    "outputId": "c6dfab04-d90b-4093-ca98-aa9b814ae1e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('User', 'what is transformer'), ('Bot', 'what is transformer in machine learning\\n\\nTransformer is a type of neural network architecture used in machine learning, particularly in natural language processing (NLP). It is designed to handle sequential data, such as text, by using self-attention mechanisms. The key components of the Transformer model include the encoder and the decoder, each of which processes the input data in a way that allows the model to capture long-range dependencies between words.\\n\\nThe Transformer architecture is different from the RNNs and LSTMs used in previous models because it uses self-attention, which allows the model to focus on the most relevant parts of the input at any given time. This makes the Transformer more efficient and capable of handling longer sequences of text compared to RNNs and LSTMs.\\n\\nIn')]\n"
     ]
    }
   ],
   "source": [
    "conversation = []\n",
    "bot_response = Qwen3_bot(\n",
    "    user_message,\n",
    "    temperature=0.1,\n",
    "    top_p=0.9,\n",
    "    max_new_tokens=150,\n",
    "    repetition_penalty=1.2\n",
    ")\n",
    "conversation.append((\"User\", user_message))\n",
    "conversation.append((\"Bot\", bot_response[0]['generated_text']))\n",
    "print(conversation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "uIPsxF48NcN6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 127
    },
    "id": "uIPsxF48NcN6",
    "outputId": "5d826899-0427-411f-95d0-861915ec3ea5"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'what is transformer in machine learning\\n\\nTransformer is a type of neural network architecture used in machine learning, particularly in natural language processing (NLP). It is designed to handle sequential data, such as text, by using self-attention mechanisms. The key components of the Transformer model include the encoder and the decoder, each of which processes the input data in a way that allows the model to capture long-range dependencies between words.\\n\\nThe Transformer architecture is different from the RNNs and LSTMs used in previous models because it uses self-attention, which allows the model to focus on the most relevant parts of the input at any given time. This makes the Transformer more efficient and capable of handling longer sequences of text compared to RNNs and LSTMs.\\n\\nIn'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bot_response[0]['generated_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RDd4WbY-DnT9",
   "metadata": {
    "id": "RDd4WbY-DnT9"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
